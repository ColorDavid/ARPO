INFO 11-04 22:01:51 __init__.py:207] Automatically detected platform cuda.
INFO 11-04 22:01:51 api_server.py:912] vLLM API server version 0.7.3
INFO 11-04 22:01:51 api_server.py:913] args: Namespace(host=None, port=9210, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='/home/dongyinpeng/mnt/models/UI-TARS-1.5-7B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt={'image': 16}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['ui-tars'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)
INFO 11-04 22:01:51 api_server.py:209] Started engine process with PID 37205
INFO 11-04 22:01:55 __init__.py:207] Automatically detected platform cuda.
INFO 11-04 22:01:57 config.py:549] This model supports multiple tasks: {'classify', 'embed', 'generate', 'reward', 'score'}. Defaulting to 'generate'.
WARNING 11-04 22:01:57 arg_utils.py:1197] The model has a long context length (128000). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 11-04 22:02:01 config.py:549] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed', 'score'}. Defaulting to 'generate'.
WARNING 11-04 22:02:01 arg_utils.py:1197] The model has a long context length (128000). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 11-04 22:02:01 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/dongyinpeng/mnt/models/UI-TARS-1.5-7B', speculative_config=None, tokenizer='/home/dongyinpeng/mnt/models/UI-TARS-1.5-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=ui-tars, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
INFO 11-04 22:02:03 cuda.py:229] Using Flash Attention backend.
INFO 11-04 22:02:04 model_runner.py:1110] Starting to load model /home/dongyinpeng/mnt/models/UI-TARS-1.5-7B...
INFO 11-04 22:02:04 config.py:3054] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]
Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:02,  2.05it/s]
Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:00<00:02,  2.42it/s]
Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:01<00:01,  2.14it/s]
Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:01<00:01,  2.01it/s]
Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:02<00:01,  1.99it/s]
Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:02<00:00,  1.94it/s]
Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:03<00:00,  1.90it/s]
Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:03<00:00,  1.98it/s]

INFO 11-04 22:02:08 model_runner.py:1115] Loading model weights took 15.6270 GB
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
WARNING 11-04 22:02:08 model_runner.py:1288] Computed max_num_seqs (min(256, 128000 // 278528)) to be less than 1. Setting it to the minimum value of 1.
It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.
Token indices sequence length is longer than the specified maximum sequence length for this model (278528 > 131072). Running this sequence through the model will result in indexing errors
WARNING 11-04 22:02:26 profiling.py:192] The context length (128000) of the model is too short to hold the multi-modal embeddings in the worst case (278528 tokens in total, out of which {'image': 262144, 'video': 16384} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
INFO 11-04 22:02:48 worker.py:267] Memory profiling takes 40.16 seconds
INFO 11-04 22:02:48 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
INFO 11-04 22:02:48 worker.py:267] model weights take 15.63GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 16.98GiB; the rest of the memory reserved for KV Cache is 38.62GiB.
INFO 11-04 22:02:48 executor_base.py:111] # cuda blocks: 45199, # CPU blocks: 4681
INFO 11-04 22:02:48 executor_base.py:116] Maximum concurrency for 128000 tokens per request: 5.65x
INFO 11-04 22:02:50 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:01<00:37,  1.10s/it]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:02<00:34,  1.05s/it]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:30,  1.05it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:03<00:26,  1.15it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:04<00:23,  1.27it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:20,  1.38it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:05<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.62it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:15,  1.72it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:13,  1.81it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:12,  1.85it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:11,  1.93it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:11,  1.99it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:10,  2.04it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:09,  2.07it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:08,  2.11it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:08,  2.16it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:10<00:07,  2.21it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:07,  2.25it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:11<00:06,  2.28it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:11<00:06,  2.31it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:12<00:05,  2.34it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:12<00:05,  2.35it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:13<00:04,  2.38it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:13<00:04,  2.40it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:14<00:03,  2.41it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:14<00:03,  2.42it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:14<00:02,  2.43it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:15<00:02,  2.44it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:15<00:02,  2.45it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:16<00:01,  2.42it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:16<00:01,  2.44it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:16<00:00,  2.38it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:17<00:00,  2.41it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:17<00:00,  2.44it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:17<00:00,  1.98it/s]
INFO 11-04 22:03:08 model_runner.py:1562] Graph capturing finished in 18 secs, took 0.37 GiB
INFO 11-04 22:03:08 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 60.35 seconds
INFO 11-04 22:03:09 api_server.py:958] Starting vLLM API server on http://0.0.0.0:9210
INFO 11-04 22:03:09 launcher.py:23] Available routes are:
INFO 11-04 22:03:09 launcher.py:31] Route: /openapi.json, Methods: HEAD, GET
INFO 11-04 22:03:09 launcher.py:31] Route: /docs, Methods: HEAD, GET
INFO 11-04 22:03:09 launcher.py:31] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 11-04 22:03:09 launcher.py:31] Route: /redoc, Methods: HEAD, GET
INFO 11-04 22:03:09 launcher.py:31] Route: /health, Methods: GET
INFO 11-04 22:03:09 launcher.py:31] Route: /ping, Methods: POST, GET
INFO 11-04 22:03:09 launcher.py:31] Route: /tokenize, Methods: POST
INFO 11-04 22:03:09 launcher.py:31] Route: /detokenize, Methods: POST
INFO 11-04 22:03:09 launcher.py:31] Route: /v1/models, Methods: GET
INFO 11-04 22:03:09 launcher.py:31] Route: /version, Methods: GET
INFO 11-04 22:03:09 launcher.py:31] Route: /v1/chat/completions, Methods: POST
INFO 11-04 22:03:09 launcher.py:31] Route: /v1/completions, Methods: POST
INFO 11-04 22:03:09 launcher.py:31] Route: /v1/embeddings, Methods: POST
INFO 11-04 22:03:09 launcher.py:31] Route: /pooling, Methods: POST
INFO 11-04 22:03:09 launcher.py:31] Route: /score, Methods: POST
INFO 11-04 22:03:09 launcher.py:31] Route: /v1/score, Methods: POST
INFO 11-04 22:03:09 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST
INFO 11-04 22:03:09 launcher.py:31] Route: /rerank, Methods: POST
INFO 11-04 22:03:09 launcher.py:31] Route: /v1/rerank, Methods: POST
INFO 11-04 22:03:09 launcher.py:31] Route: /v2/rerank, Methods: POST
INFO 11-04 22:03:09 launcher.py:31] Route: /invocations, Methods: POST
INFO:     Started server process [37035]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
Stopping all processes...
INFO 11-04 22:04:54 launcher.py:62] Shutting down FastAPI HTTP server.
[rank0]:[W1104 22:04:55.398327159 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO 11-04 22:05:09 __init__.py:207] Automatically detected platform cuda.
INFO 11-04 22:05:09 api_server.py:912] vLLM API server version 0.7.3
INFO 11-04 22:05:09 api_server.py:913] args: Namespace(host=None, port=9210, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='/home/dongyinpeng/mnt/models/UI-TARS-1.5-7B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt={'image': 16}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['ui-tars'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)
INFO 11-04 22:05:09 api_server.py:209] Started engine process with PID 47143
INFO 11-04 22:05:13 __init__.py:207] Automatically detected platform cuda.
INFO 11-04 22:05:16 config.py:549] This model supports multiple tasks: {'reward', 'embed', 'score', 'classify', 'generate'}. Defaulting to 'generate'.
WARNING 11-04 22:05:16 arg_utils.py:1197] The model has a long context length (128000). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 11-04 22:05:21 config.py:549] This model supports multiple tasks: {'score', 'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.
WARNING 11-04 22:05:21 arg_utils.py:1197] The model has a long context length (128000). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 11-04 22:05:21 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/dongyinpeng/mnt/models/UI-TARS-1.5-7B', speculative_config=None, tokenizer='/home/dongyinpeng/mnt/models/UI-TARS-1.5-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=ui-tars, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
INFO 11-04 22:05:22 cuda.py:229] Using Flash Attention backend.
INFO 11-04 22:05:23 model_runner.py:1110] Starting to load model /home/dongyinpeng/mnt/models/UI-TARS-1.5-7B...
INFO 11-04 22:05:23 config.py:3054] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]
Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:03,  1.99it/s]
Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:00<00:02,  2.38it/s]
Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:01<00:01,  2.09it/s]
Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:01<00:01,  1.98it/s]
Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:02<00:01,  1.98it/s]
Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:03<00:00,  1.90it/s]
Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:03<00:00,  1.88it/s]
Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:03<00:00,  1.96it/s]

INFO 11-04 22:05:27 model_runner.py:1115] Loading model weights took 15.6270 GB
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
WARNING 11-04 22:05:27 model_runner.py:1288] Computed max_num_seqs (min(256, 128000 // 278528)) to be less than 1. Setting it to the minimum value of 1.
It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.
Token indices sequence length is longer than the specified maximum sequence length for this model (278528 > 131072). Running this sequence through the model will result in indexing errors
WARNING 11-04 22:05:44 profiling.py:192] The context length (128000) of the model is too short to hold the multi-modal embeddings in the worst case (278528 tokens in total, out of which {'image': 262144, 'video': 16384} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
INFO 11-04 22:06:06 worker.py:267] Memory profiling takes 38.79 seconds
INFO 11-04 22:06:06 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
INFO 11-04 22:06:06 worker.py:267] model weights take 15.63GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 16.98GiB; the rest of the memory reserved for KV Cache is 38.62GiB.
INFO 11-04 22:06:06 executor_base.py:111] # cuda blocks: 45199, # CPU blocks: 4681
INFO 11-04 22:06:06 executor_base.py:116] Maximum concurrency for 128000 tokens per request: 5.65x
INFO 11-04 22:06:08 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:16,  2.10it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:16,  2.04it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:15,  2.04it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:15,  2.05it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:14,  2.09it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:13,  2.15it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:12,  2.23it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:03<00:11,  2.26it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:04<00:11,  2.29it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:04<00:10,  2.30it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:04<00:10,  2.32it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:05<00:09,  2.35it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:05<00:09,  2.36it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:06<00:08,  2.37it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:06<00:08,  2.38it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:07<00:07,  2.39it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:07<00:07,  2.42it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:07<00:07,  2.43it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:08<00:06,  2.44it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:08<00:06,  2.46it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:09<00:05,  2.47it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:09<00:05,  2.47it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:09<00:04,  2.48it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:10<00:04,  2.49it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:10<00:04,  2.49it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:11<00:03,  2.49it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:11<00:03,  2.49it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:11<00:02,  2.49it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:12<00:02,  2.44it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:12<00:02,  2.46it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:13<00:01,  2.47it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:13<00:01,  2.46it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:13<00:00,  2.44it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:14<00:00,  2.44it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.45it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.37it/s]
INFO 11-04 22:06:23 model_runner.py:1562] Graph capturing finished in 15 secs, took 0.37 GiB
INFO 11-04 22:06:23 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 56.05 seconds
INFO 11-04 22:06:24 api_server.py:958] Starting vLLM API server on http://0.0.0.0:9210
INFO 11-04 22:06:24 launcher.py:23] Available routes are:
INFO 11-04 22:06:24 launcher.py:31] Route: /openapi.json, Methods: GET, HEAD
INFO 11-04 22:06:24 launcher.py:31] Route: /docs, Methods: GET, HEAD
INFO 11-04 22:06:24 launcher.py:31] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 11-04 22:06:24 launcher.py:31] Route: /redoc, Methods: GET, HEAD
INFO 11-04 22:06:24 launcher.py:31] Route: /health, Methods: GET
INFO 11-04 22:06:24 launcher.py:31] Route: /ping, Methods: GET, POST
INFO 11-04 22:06:24 launcher.py:31] Route: /tokenize, Methods: POST
INFO 11-04 22:06:24 launcher.py:31] Route: /detokenize, Methods: POST
INFO 11-04 22:06:24 launcher.py:31] Route: /v1/models, Methods: GET
INFO 11-04 22:06:24 launcher.py:31] Route: /version, Methods: GET
INFO 11-04 22:06:24 launcher.py:31] Route: /v1/chat/completions, Methods: POST
INFO 11-04 22:06:24 launcher.py:31] Route: /v1/completions, Methods: POST
INFO 11-04 22:06:24 launcher.py:31] Route: /v1/embeddings, Methods: POST
INFO 11-04 22:06:24 launcher.py:31] Route: /pooling, Methods: POST
INFO 11-04 22:06:24 launcher.py:31] Route: /score, Methods: POST
INFO 11-04 22:06:24 launcher.py:31] Route: /v1/score, Methods: POST
INFO 11-04 22:06:24 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST
INFO 11-04 22:06:24 launcher.py:31] Route: /rerank, Methods: POST
INFO 11-04 22:06:24 launcher.py:31] Route: /v1/rerank, Methods: POST
INFO 11-04 22:06:24 launcher.py:31] Route: /v2/rerank, Methods: POST
INFO 11-04 22:06:24 launcher.py:31] Route: /invocations, Methods: POST
INFO:     Started server process [46930]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
Stopping all processes...
INFO 11-04 22:12:50 launcher.py:62] Shutting down FastAPI HTTP server.
[rank0]:[W1104 22:12:51.379453961 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO 11-04 22:13:00 __init__.py:207] Automatically detected platform cuda.
INFO 11-04 22:13:00 api_server.py:912] vLLM API server version 0.7.3
INFO 11-04 22:13:00 api_server.py:913] args: Namespace(host=None, port=9000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='/home/dongyinpeng/mnt/models/UI-TARS-1.5-7B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt={'image': 16}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['ui-tars'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)
INFO 11-04 22:13:00 api_server.py:209] Started engine process with PID 31145
INFO 11-04 22:13:04 __init__.py:207] Automatically detected platform cuda.
INFO 11-04 22:13:06 config.py:549] This model supports multiple tasks: {'reward', 'generate', 'score', 'classify', 'embed'}. Defaulting to 'generate'.
WARNING 11-04 22:13:06 arg_utils.py:1197] The model has a long context length (128000). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 11-04 22:13:10 config.py:549] This model supports multiple tasks: {'classify', 'reward', 'generate', 'score', 'embed'}. Defaulting to 'generate'.
WARNING 11-04 22:13:10 arg_utils.py:1197] The model has a long context length (128000). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 11-04 22:13:10 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/dongyinpeng/mnt/models/UI-TARS-1.5-7B', speculative_config=None, tokenizer='/home/dongyinpeng/mnt/models/UI-TARS-1.5-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=ui-tars, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
INFO 11-04 22:13:12 cuda.py:229] Using Flash Attention backend.
INFO 11-04 22:13:12 model_runner.py:1110] Starting to load model /home/dongyinpeng/mnt/models/UI-TARS-1.5-7B...
INFO 11-04 22:13:12 config.py:3054] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]
Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:03,  1.94it/s]
Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:00<00:02,  2.36it/s]
Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:01<00:01,  2.17it/s]
Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:01<00:01,  2.04it/s]
Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:02<00:00,  2.08it/s]
Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:02<00:00,  1.96it/s]
Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:03<00:00,  1.92it/s]
Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:03<00:00,  2.00it/s]

INFO 11-04 22:13:16 model_runner.py:1115] Loading model weights took 15.6270 GB
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
WARNING 11-04 22:13:17 model_runner.py:1288] Computed max_num_seqs (min(256, 128000 // 278528)) to be less than 1. Setting it to the minimum value of 1.
It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.
Token indices sequence length is longer than the specified maximum sequence length for this model (278528 > 131072). Running this sequence through the model will result in indexing errors
WARNING 11-04 22:13:34 profiling.py:192] The context length (128000) of the model is too short to hold the multi-modal embeddings in the worst case (278528 tokens in total, out of which {'image': 262144, 'video': 16384} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
INFO 11-04 22:13:56 worker.py:267] Memory profiling takes 40.05 seconds
INFO 11-04 22:13:56 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
INFO 11-04 22:13:56 worker.py:267] model weights take 15.63GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 16.98GiB; the rest of the memory reserved for KV Cache is 38.62GiB.
INFO 11-04 22:13:57 executor_base.py:111] # cuda blocks: 45199, # CPU blocks: 4681
INFO 11-04 22:13:57 executor_base.py:116] Maximum concurrency for 128000 tokens per request: 5.65x
INFO 11-04 22:13:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:15,  2.17it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:14,  2.29it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:13,  2.32it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:13,  2.35it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:12,  2.36it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:12,  2.37it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:02<00:11,  2.38it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:03<00:11,  2.37it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:03<00:10,  2.37it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:04<00:10,  2.38it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:04<00:10,  2.38it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:05<00:09,  2.38it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:05<00:09,  2.38it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:05<00:08,  2.39it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:06<00:08,  2.40it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:06<00:07,  2.40it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:07<00:07,  2.42it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:07<00:06,  2.44it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:07<00:06,  2.45it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:08<00:06,  2.45it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:08<00:05,  2.45it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:09<00:05,  2.46it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:09<00:04,  2.47it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:09<00:04,  2.47it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:10<00:04,  2.47it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:10<00:03,  2.48it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:11<00:03,  2.47it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:11<00:02,  2.47it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:12<00:02,  2.41it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:12<00:02,  2.43it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:12<00:01,  2.44it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:13<00:01,  2.42it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:13<00:00,  2.40it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:14<00:00,  2.42it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.42it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.41it/s]
INFO 11-04 22:14:13 model_runner.py:1562] Graph capturing finished in 15 secs, took 0.37 GiB
INFO 11-04 22:14:13 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 57.00 seconds
INFO 11-04 22:14:14 api_server.py:958] Starting vLLM API server on http://0.0.0.0:9000
INFO 11-04 22:14:14 launcher.py:23] Available routes are:
INFO 11-04 22:14:14 launcher.py:31] Route: /openapi.json, Methods: GET, HEAD
INFO 11-04 22:14:14 launcher.py:31] Route: /docs, Methods: GET, HEAD
INFO 11-04 22:14:14 launcher.py:31] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 11-04 22:14:14 launcher.py:31] Route: /redoc, Methods: GET, HEAD
INFO 11-04 22:14:14 launcher.py:31] Route: /health, Methods: GET
INFO 11-04 22:14:14 launcher.py:31] Route: /ping, Methods: GET, POST
INFO 11-04 22:14:14 launcher.py:31] Route: /tokenize, Methods: POST
INFO 11-04 22:14:14 launcher.py:31] Route: /detokenize, Methods: POST
INFO 11-04 22:14:14 launcher.py:31] Route: /v1/models, Methods: GET
INFO 11-04 22:14:14 launcher.py:31] Route: /version, Methods: GET
INFO 11-04 22:14:14 launcher.py:31] Route: /v1/chat/completions, Methods: POST
INFO 11-04 22:14:14 launcher.py:31] Route: /v1/completions, Methods: POST
INFO 11-04 22:14:14 launcher.py:31] Route: /v1/embeddings, Methods: POST
INFO 11-04 22:14:14 launcher.py:31] Route: /pooling, Methods: POST
INFO 11-04 22:14:14 launcher.py:31] Route: /score, Methods: POST
INFO 11-04 22:14:14 launcher.py:31] Route: /v1/score, Methods: POST
INFO 11-04 22:14:14 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST
INFO 11-04 22:14:14 launcher.py:31] Route: /rerank, Methods: POST
INFO 11-04 22:14:14 launcher.py:31] Route: /v1/rerank, Methods: POST
INFO 11-04 22:14:14 launcher.py:31] Route: /v2/rerank, Methods: POST
INFO 11-04 22:14:14 launcher.py:31] Route: /invocations, Methods: POST
INFO:     Started server process [30912]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
Stopping all processes...
INFO 11-04 22:17:33 launcher.py:62] Shutting down FastAPI HTTP server.
[rank0]:[W1104 22:17:33.716747001 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
